{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNet/Gluon Bootcamp\n",
    "### Day 1\n",
    "| Title | Description | Duration | Presenter| \n",
    "|:---    |:---   |:---         |:---      |\n",
    "|Sage Maker Architecture|A deep dive into SageMaker SDK and security model|30 min|Cristian|\n",
    "|DevOps with SageMaker|A deep dive into how to integrage SageMaker into development pipeline for teams of data scientists|30 min|Christian|\n",
    "|MXNet overview|an overview of MXNet architecture and components|30 min|Thom|\n",
    "|First glance at Gluon|Walking through a simple regression model to get an understanding of gluon components|30 min|Cyrus|\n",
    "|LAB: Interactive GLuon crash course|A step-by-step implementation of a conv net using multiple components of Gluon|120 min|Thom, Cyrus|\n",
    "|Multi GPU Training in Gluon|An introduction to training using multiple GPUs in Gluon |15 min|Eden|\n",
    "|LAB: Multi GPU Training in Gluon|A hands-on lab to get started training using multiple GPUs with Gluon|30 min|Eden|\n",
    "|Distributed Training with MXNet|A deeper dive into cluster setup, sizing, optimization algorithms, and other considerations for distributed training|60 min|Thom, Eden, Cyrus|\n",
    "|LAB: Distributed training with SageMaker and Gluon|Participants learn the simple steps to training using a cluster with Gluon and Sagemaker|30 min|Eden|\n",
    "\n",
    "### Day2\n",
    "| Title | Description | Duration | Presenter| \n",
    "|:---    |:---   |:---         |:---      |\n",
    "|Getting Started with Deep Learning AMI|The particpants will be introduced to DLAMI and versions of frameworks it supports|30 min|Christian|\n",
    "|LAB:Customized DataSets and Data Loaders in Gluon|Participants will learn hour to derive customized datasets from DataSet class to fit their requirements|30 min|Thom|\n",
    "|LAB: Block and Hybrid BLock|Participants learn how to created customized blocks in order to create flexible and non-sequential models|30 min|Thom|\n",
    "|MXNBoard and Profiling|This module includes how to profile MXNet using pycharm professional and detect performance bottlenecks|30 min|Thom|\n",
    "|LAB: Implement a simple LSTM|Participant in this section implement a simple LSTM network|60 min|Cyrus|\n",
    "|Optimizing LSTM|Optimizing training performance of LSTM network in multi-device setting using MXBoard|30 min|Cyrus, Thom|\n",
    "|Theory of LSTNet|This section describes theory of Long and Short Term Temporal Patterns with Deep Neural Networks|30 min|Cyrus|\n",
    "|LAB: Implementing LSTNet|A code walk through will be presented to the audiance as how LSTNet is implementd in Gluon|90 min|Thom|\n",
    "|LAB: Optimizing your LSTNet for multi-device training|converting your code for multi-device training and then using optimization ideas to increased training speed|60 min|Thom and Cyrus|\n",
    "|Hybredize|At this point we change training mode from imperative to symbolic and traing the LSTNet model on multi-device setting in symbolic mode|30 min|Thom|\n",
    "\n",
    "### Day3\n",
    "| Title | Description | Duration | Presenter| \n",
    "|:---    |:---   |:---         |:---      |\n",
    "|LAB: Back to Amazon SageMaker|THe LSTNet code will be ported to train on SageMaker using the Python SDK|90 min|Eden, Christian|\n",
    "|LAB: Distributed and multi-GPU LSTNet on SageMaker|Participants will speed up the training of the LSTNet model with data distribution across multiple GPUs and hosts|30 min|Eden|\n",
    "|HyperParameterOptimization, the theory|SageMaker can optionally use Bayesian HPO for training optimization. This module describes the science of Bayesian HPO. Additionally we shall demonstrate how to use HPO for training of a simple gluon model in SageMaker|60 min|Thom|\n",
    "|LAB:Deploying an end point using Amazon SageMaker|We discover three scenarios, Deploying a model developed in SageMaker, Deploying model artefacts from S3, and deploying model artefacts from docker image|60 min|Eden and Christian|\n",
    "|MXNet on Edge, Pi3 version|In this module we demonstrate a simple model that runs on RPI3|30 min|Christian|\n",
    "|MXNet on Edge, Amazon GreenGrass|In this module we demonstrate deployment of a simple computer vision model on Amazon GreenGrass using Lambda|30 min|Christian|\n",
    "|LAB: Bidirectional LSTM using Gluon|In this module We extende our original simple LSTM to become bi-directional|30 min|Cyrus and Thom|\n",
    "|LAB: Bidirectional LSTM using Keras 2 and MXNet|In this module we use Keras to implement a bidirection LSTM using Keras and MXNet backend|30 min|Cyrus and Thom|\n",
    "|Advaned GLuon Linraries|Libraries such as gluonCV and gluon NLP will explained to familiarize the participants with the lastest releases.|30 min|Thom|\n",
    "\n",
    "### Day4\n",
    "| Title | Description | Duration | Presenter| \n",
    "|:---    |:---   |:---         |:---      |\n",
    "|Hackathon|A project based on bi-directional LSTM will be implemented to incorporate all the material from the first three days. If participants have prepared datasets of significant size we attempt to optimize the distributed training for large batch sizes|7 hours|all|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !notedown README.ipynb --to markdown > README.MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
