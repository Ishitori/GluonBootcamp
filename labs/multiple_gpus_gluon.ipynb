{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lab: Training on multiple GPUs with `gluon`\n",
    "\n",
    "This lab demonstrates the concepts of how to split up the training of a model across multiple GPUs using Gluon. Dat a parallelism will be used where each batch is split into equal portions, a forward and backward pass is performed, and the gradients are summed and the parameters are updated. A complete copy of all the parameters is present on each GPU.\n",
    "\n",
    "This lab has been adapted from https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html.\n",
    "\n",
    "This lab has been tested using a ml.p3.8xlarge SageMaker notebook instance. It requires an instance with multiple GPUs.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "* Choose “local“ or “device“ kvstore\n",
    "\n",
    "* Initialise the parameters and copy all of them to each GPU\n",
    "* Split up the batch into portions and copy each portion onto a GPU\n",
    "* Run forward and backward\n",
    "\n",
    "The following steps are run automatically by Gluon when parameters on multiple devices are detected:\n",
    "* Sum the gradients across all GPUs and broadcast to all GPUs\n",
    "* Update the weights\n",
    "\n",
    "Start by defining a simple convolutional neural network for image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "net = gluon.nn.Sequential(prefix='cnn_')\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "    \n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize on multiple devices\n",
    "\n",
    "Gluon supports initialization of network parameters over multiple devices. This is done by passing in an array of device contexts, instead of a single context.\n",
    "When we pass in an array of contexts, the parameters are initialized \n",
    "to be identical across all of our devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 2\n",
    "ctx = [mx.gpu(i) for i in range(GPU_COUNT)]\n",
    "net.collect_params().initialize(ctx=ctx,force_reinit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch of input data is split into parts (one for each GPU) \n",
    "by calling `gluon.utils.split_and_load(batch, ctx)`.\n",
    "The `split_and_load` function also loads each part onto the appropriate device context. \n",
    "\n",
    "When the forward and backwards passes are computed later, this is executed on the device with the version of the parameters which have been stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.00482063  0.00191563  0.02079182 -0.01033005  0.01591893 -0.00201501\n",
      "  -0.00970065 -0.0108907  -0.00311239  0.0009627 ]\n",
      " [ 0.00226819 -0.00864111  0.01155005 -0.03112838  0.04687387 -0.01447218\n",
      "  -0.00345993 -0.01687234 -0.00883009 -0.00482067]]\n",
      "<NDArray 2x10 @gpu(0)>\n",
      "\n",
      "[[-0.01494103  0.00641203  0.02740048 -0.01331081  0.01718488 -0.02049927\n",
      "  -0.00956308 -0.01934761  0.00790679 -0.0114479 ]\n",
      " [-0.00764965  0.00055102  0.00661528 -0.01601085  0.02302293 -0.00435258\n",
      "  -0.01212662 -0.01362745  0.00415612 -0.00635127]]\n",
      "<NDArray 2x10 @gpu(1)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet.test_utils import get_mnist\n",
    "mnist = get_mnist()\n",
    "batch = mnist['train_data'][0:GPU_COUNT*2, :]\n",
    "data = gluon.utils.split_and_load(batch, ctx)\n",
    "print(net(data[0]))\n",
    "print(net(data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any time, we can access the version of the parameters stored on each device. \n",
    "Recall from the first Chapter that our weights may not actually be initialized\n",
    "when we call `initialize` because the parameter shapes may not yet be known. \n",
    "In these cases, initialization is deferred pending shape inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== channel 0 of the first conv on gpu(0) ===\n",
      "[[[ 0.0068339   0.01299825  0.0301265 ]\n",
      "  [ 0.04819721  0.01438687  0.05011239]\n",
      "  [ 0.00628365  0.04861524 -0.01068833]]]\n",
      "<NDArray 1x3x3 @gpu(0)>\n",
      "=== channel 0 of the first conv on gpu(1) ===\n",
      "[[[ 0.0068339   0.01299825  0.0301265 ]\n",
      "  [ 0.04819721  0.01438687  0.05011239]\n",
      "  [ 0.00628365  0.04861524 -0.01068833]]]\n",
      "<NDArray 1x3x3 @gpu(1)>\n"
     ]
    }
   ],
   "source": [
    "weight = net.collect_params()['cnn_conv0_weight']\n",
    "\n",
    "for c in ctx:\n",
    "    print('=== channel 0 of the first conv on {} ==={}'.format(\n",
    "        c, weight.data(ctx=c)[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can access the gradients on each of the GPUs. Because each GPU gets a different part of the batch (a different subset of examples), the gradients on each GPU vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== grad of channel 0 of the first conv2d on gpu(0) ===\n",
      "[[[ 0.01843166 -0.007361   -0.01329759]\n",
      "  [ 0.00464045 -0.00820222 -0.01046472]\n",
      "  [ 0.02330405 -0.00166359 -0.01982304]]]\n",
      "<NDArray 1x3x3 @gpu(0)>\n",
      "=== grad of channel 0 of the first conv2d on gpu(1) ===\n",
      "[[[-0.07402541 -0.06295483 -0.02819332]\n",
      "  [-0.08098998 -0.06217923 -0.01296097]\n",
      "  [-0.03471691 -0.02846127 -0.0076396 ]]]\n",
      "<NDArray 1x3x3 @gpu(1)>\n"
     ]
    }
   ],
   "source": [
    "def forward_backward(net, data, label):\n",
    "    with autograd.record():\n",
    "        losses = [loss(net(X), Y) for X, Y in zip(data, label)]\n",
    "    for l in losses:\n",
    "        l.backward()\n",
    "        \n",
    "label = gluon.utils.split_and_load(mnist['train_label'][0:4], ctx)\n",
    "forward_backward(net, data, label)\n",
    "for c in ctx:\n",
    "    print('=== grad of channel 0 of the first conv2d on {} ==={}'.format(\n",
    "        c, weight.grad(ctx=c)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all things together\n",
    "\n",
    "Now we can implement the remaining functions. Most of them are the same as [when we did everything by hand](./chapter07_distributed-learning/multiple-gpus-scratch.ipynb); one notable difference is that if a `gluon` trainer recognizes multi-devices, it will automatically aggregate the gradients and synchronize the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on [gpu(0)]\n",
      "Batch size is 64\n",
      "Epoch 0, training time = 2.3 sec\n",
      "         validation accuracy = 0.9733\n",
      "Epoch 1, training time = 2.2 sec\n",
      "         validation accuracy = 0.9846\n",
      "Epoch 2, training time = 2.3 sec\n",
      "         validation accuracy = 0.9877\n",
      "Epoch 3, training time = 2.2 sec\n",
      "         validation accuracy = 0.9881\n",
      "Epoch 4, training time = 2.2 sec\n",
      "         validation accuracy = 0.9866\n",
      "Running on [gpu(0), gpu(1)]\n",
      "Batch size is 128\n",
      "Epoch 0, training time = 2.3 sec\n",
      "         validation accuracy = 0.9508\n",
      "Epoch 1, training time = 2.4 sec\n",
      "         validation accuracy = 0.9639\n",
      "Epoch 2, training time = 2.4 sec\n",
      "         validation accuracy = 0.9821\n",
      "Epoch 3, training time = 2.3 sec\n",
      "         validation accuracy = 0.9860\n",
      "Epoch 4, training time = 2.3 sec\n",
      "         validation accuracy = 0.9873\n"
     ]
    }
   ],
   "source": [
    "from mxnet.io import NDArrayIter\n",
    "from time import time\n",
    "\n",
    "def train_batch(batch, ctx, net, trainer):\n",
    "    # split the data batch and load them on GPUs\n",
    "    data = gluon.utils.split_and_load(batch.data[0], ctx)\n",
    "    label = gluon.utils.split_and_load(batch.label[0], ctx)\n",
    "    # compute gradient\n",
    "    forward_backward(net, data, label)\n",
    "    # update parameters\n",
    "    trainer.step(batch.data[0].shape[0])\n",
    "    \n",
    "def valid_batch(batch, ctx, net):\n",
    "    data = batch.data[0].as_in_context(ctx[0])\n",
    "    pred = nd.argmax(net(data), axis=1)\n",
    "    return nd.sum(pred == batch.label[0].as_in_context(ctx[0])).asscalar()    \n",
    "\n",
    "def run(num_gpus, batch_size, lr):    \n",
    "    # the list of GPUs will be used\n",
    "    ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "    print('Running on {}'.format(ctx))\n",
    "    \n",
    "    # data iterator\n",
    "    mnist = get_mnist()\n",
    "    train_data = NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"], batch_size)\n",
    "    valid_data = NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size)\n",
    "    print('Batch size is {}'.format(batch_size))\n",
    "    \n",
    "    net.collect_params().initialize(force_reinit=True, ctx=ctx)\n",
    "    # Here the kvstore can be set to 'local' where the gradients are summed and synced on the cpu\n",
    "    # or 'device' on the GPUs. If 'device' is selected mxnet uses GPU to GPU comms where possible.\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr}, kvstore='local')\n",
    "    for epoch in range(5):\n",
    "        # train\n",
    "        start = time()\n",
    "        train_data.reset()\n",
    "        for batch in train_data:\n",
    "            train_batch(batch, ctx, net, trainer)\n",
    "        nd.waitall()  # wait until all computations are finished to benchmark the time\n",
    "        print('Epoch %d, training time = %.1f sec'%(epoch, time()-start))\n",
    "        \n",
    "        # validating\n",
    "        valid_data.reset()\n",
    "        correct, num = 0.0, 0.0\n",
    "        for batch in valid_data:\n",
    "            correct += valid_batch(batch, ctx, net)\n",
    "            num += batch.data[0].shape[0]                \n",
    "        print('         validation accuracy = %.4f'%(correct/num))\n",
    "        \n",
    "run(1, 64, .3)        \n",
    "run(GPU_COUNT, 64*GPU_COUNT, .3) # a larger batch size is used so each GPU has enough data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example the network is relatively small. This makes the communication overhead higher than the computational gain from having 2 GPUs, hence there is no speed up. The network also takes longer to converge due to the larger batch size."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
