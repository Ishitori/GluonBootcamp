{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training with Amazon SageMaker and `gluon`\n",
    "\n",
    "This lab demonstrates how to perform distributed training on multiple hosts using Gluon and Amazon SageMaker. \n",
    "\n",
    "A multiple regression model will be trained on a dataset of synthetic examples. A single dense layer will be used, optimised via the `gluon` stochastic gradient descent Trainer. The function to be learned is:\n",
    "\n",
    "`y = 2*x0 - 3.4*x1 + 4.2`\n",
    "\n",
    "Where y is in R and x = (x1,x2) is in R2\n",
    "\n",
    "The synthetic dataset has 10 million examples and a 1% test set is held out.\n",
    "\n",
    "There are two main steps to distribute this workload on multiple hosts using `gluon`:\n",
    "\n",
    "1. Choose the version of kvstore to use when creating the Gluon Trainer. For distributed training it is either 'dist_sync', 'dist_device_sync', 'dist_async'. See there refence (https://mxnet.incubator.apache.org/api/python/kvstore/kvstore.html#mxnet.kvstore.create) for details.\n",
    "2. Specify more than 1 instance when creating a Amazon SageMaker MXNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up environment variables\n",
    "\n",
    "MXNet, boto3, and sagemaker are imported. Fill in the bucket name for a bucket that exists in your account and has access from the role which the Amazon SageMaker notebook instance takes on. Optionally chagne the prefix of the folder in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "import os\n",
    "import boto3\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# specify your bucket name here which is accessible from the SageMaker notebook\n",
    "bucket_name = 'eduthie-sagemaker-1'\n",
    "# an prefix for the folder name for this lab, include the trailing slash\n",
    "prefix = 'distributed_training_gluon_lab/'\n",
    "\n",
    "local_dir = '/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function to learn and generate data\n",
    "\n",
    "The function real_fn is defined and a synthetic dataset of 10 million examples is created. Random noise is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 1000000\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "\n",
    "X = nd.random_normal(shape=(num_examples, num_inputs))\n",
    "noise = 0.01 * nd.random_normal(shape=(num_examples,))\n",
    "y = real_fn(X) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to split and uplaod the dataset to Amazon S3\n",
    "\n",
    "The data is uploaded to Amazon S3 so it can be used to train in Amazon SageMaker. Full copies of the dataset are uploaded as a single file to 'train/full' to be trained on a single host. The data is sharded into 5 parts and uploaded to 'train' in order to run on 5 hosts. A complete copy of the test 1% is uploaded to 'test'.\n",
    "\n",
    "Data parallelism is used here. The dataset is split into 5 parts. The 5 separate hosts work on one of these parts. During each epoch, on each host, the part is split up into batches. The data is propogated forwards and backwards through the network to calculate the gradients for each batch. At the end of the batch the gradients are summed over the 5 hosts and broadcast back using the kvstore. \n",
    "\n",
    "This is achieved by setting the kvstore parameter of the gluon Trainer to 'dist_device_sync'. If 'dist_async' is used the training on each host does not wait for the gradients on other hosts to complete before proceeding to the bext batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the ndarrays X and y in a dictionary in a file\n",
    "# and then uploads them to the target_folder in Amazon S3 with a filename i\n",
    "def save_and_upload(X,y,target_folder,i):\n",
    "    file_name = '{}'.format(i)\n",
    "    local_path = os.path.join(local_dir,file_name)\n",
    "    mx.nd.save(local_path,{'X':X, 'y':y})\n",
    "    print('Number of examples {}'.format(len(X)))\n",
    "    print('Created local file {}'.format(local_path))\n",
    "    upload_filename = '{}{}/{}'.format(prefix,target_folder,file_name)\n",
    "    print('Uploading to {}'.format(upload_filename))\n",
    "    s3.upload_file(local_path, bucket_name, upload_filename)\n",
    "\n",
    "# Splits the ndarrays X and y into k equal shards and saves each to a local file\n",
    "# each file is uploaded to Amazon S3 in the target_folder named by inded starting from 0\n",
    "def split_and_upload(X,y,k,target_folder):\n",
    "    n = len(X)\n",
    "    assert (n//k)*k == n\n",
    "    idx = list(range(0, n+1, n//k))\n",
    "    X_shards = [X[idx[i]:idx[i+1]] for i in range(k)]\n",
    "    y_shards = [y[idx[i]:idx[i+1]] for i in range(k)]\n",
    "    \n",
    "    for Xi,yi,i in zip(X_shards,y_shards,range(k)):\n",
    "        save_and_upload(Xi,yi,target_folder,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into training and test, split into shards, and uploaded to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000\n",
      "100000\n",
      "Number of examples 900000\n",
      "Created local file /tmp/0\n",
      "Uploading to distributed_training_gluon_lab/train/full/0\n",
      "Number of examples 180000\n",
      "Created local file /tmp/0\n",
      "Uploading to distributed_training_gluon_lab/train/0\n",
      "Number of examples 180000\n",
      "Created local file /tmp/1\n",
      "Uploading to distributed_training_gluon_lab/train/1\n",
      "Number of examples 180000\n",
      "Created local file /tmp/2\n",
      "Uploading to distributed_training_gluon_lab/train/2\n",
      "Number of examples 180000\n",
      "Created local file /tmp/3\n",
      "Uploading to distributed_training_gluon_lab/train/3\n",
      "Number of examples 180000\n",
      "Created local file /tmp/4\n",
      "Uploading to distributed_training_gluon_lab/train/4\n",
      "Number of examples 100000\n",
      "Created local file /tmp/0\n",
      "Uploading to distributed_training_gluon_lab/test/0\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.9\n",
    "num_splits = 5\n",
    "split_index = int(num_examples*train_frac)\n",
    "X_train = X[0:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_train = y[0:split_index]\n",
    "y_test = y[split_index:]\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "save_and_upload(X_train,y_train,'train/full',0)\n",
    "split_and_upload(X_train,y_train,num_splits,'train')\n",
    "save_and_upload(X_test,y_test,'test',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the python module and test locally\n",
    "\n",
    "To train a custom MXNet model in Amazon SageMaker, the model code is uploaded in a separate python module. This is specified using the 'entry_point' parameter referencing a local path. That module requires a train() method. See multiple_regession.py for more details.\n",
    "\n",
    "If any additional code or libraries are referenced in the entry_point module they can also be uploaded to the training instances using the 'source_dir' parameter. All contents of source_dir are copied to the training server recursively, and the training script uses that director as a working directory when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_regression import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file path ./data/0\n",
      "Number of examples 100\n",
      "kvstore device\n"
     ]
    }
   ],
   "source": [
    "channel_input_dirs = {'train':'./data'}\n",
    "hyperparameters = {'batch_size':64, 'epochs':10, 'learning_rate':0.1}\n",
    "train(hyperparameters=hyperparameters,channel_input_dirs=channel_input_dirs,num_gpus=1,hosts=['alg-1'],\n",
    "      current_host='alg-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Amazon SageMaker\n",
    "\n",
    "Two SageMaker estimators are created using the higher level Python API MXNet class. In this case two estimators are created, specifying the train_instane_count and train_instance_type to select the number of hosts in a cluster and the type of Amazon ec2 instance used:\n",
    "\n",
    "1. A distributed version running on 5 hosts with a ml.p3.2xlarge instance.\n",
    "2. A single-host version on the same instance type.\n",
    "\n",
    "The role used is passed, which is a string of the arn of the role used by Amazon SageMaker.\n",
    "\n",
    "The hyperparameters are input as a dictionary. In this case a batch_size of 64 is used over 10 epochs, with a very small learning_rate. These are not optimal for this task, see what you can do to improve the convergence time.\n",
    "\n",
    "Finally the fit() method is called on the estimator with the training and test data as parameters. wait is set to False so fit returns immediately after the job is created. \n",
    "\n",
    "After running the cell below, go to the 'Training Jobs' section of the Amazon SageMaker console to monitor the progress of the jobs.\n",
    "\n",
    "l_estimator_1 on 1 machine takes approcimately 45 minutes to complete. Feel free to cancel either of these jobs before they finish using the console.\n",
    "\n",
    "l_estimator_5 completes in about 15 minutes on 5 machines. There is some communication overhead in distributed mode but still a substantial speedup.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1. How much overhead is there when running a distributed job on SageMaker for spinning up the machines and for syncing the parameters?\n",
    "2. What else can be done to speed up training?\n",
    "3. Why does the first epoch take twice as long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-07-13-09-24-59-913\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-07-13-09-25-01-355\n"
     ]
    }
   ],
   "source": [
    "l_estimator_5 = MXNet(entry_point='multiple_regression.py',\n",
    "    role=role,\n",
    "    train_instance_count=num_splits, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters={'batch_size':64, 'epochs':10, 'learning_rate':0.0000001, 'sync':True})\n",
    "\n",
    "l_estimator_1 = MXNet(entry_point='multiple_regression.py',\n",
    "    role=role,\n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters={'batch_size':64, 'epochs':10, 'learning_rate':0.0000001})\n",
    "\n",
    "train_data_location = 's3://{}/{}train'.format(bucket_name,prefix)\n",
    "test_data_location = 's3://{}/{}test'.format(bucket_name,prefix)\n",
    "\n",
    "l_estimator_1.fit({'train': 's3://{}/{}train/full'.format(bucket_name,prefix), \n",
    "                       'test': 's3://{}/{}test'.format(bucket_name,prefix)},wait=False)\n",
    "l_estimator_5.fit({'train': 's3://{}/{}train'.format(bucket_name,prefix), \n",
    "    'test': 's3://{}/{}test'.format(bucket_name,prefix)},wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
