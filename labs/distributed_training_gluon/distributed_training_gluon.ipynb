{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training with SageMaker and Gluon\n",
    "\n",
    "This lab demonstrates how to perform distributed training on multiple hosts using Gluon and SageMaker. There are two main steps:\n",
    "\n",
    "1. Choose the version of kvstore to use when creating the Gluon Trainer. For distributed training it is either 'dist_sync', 'dist_device_sync', 'dist_async'. See there refence (https://mxnet.incubator.apache.org/api/python/kvstore/kvstore.html#mxnet.kvstore.create) for details.\n",
    "2. Specify more than 1 instance when creating a SageMaker MXNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "import os\n",
    "import boto3\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'eduthie-sagemaker-1'\n",
    "prefix = 'distributed_training_gluon_lab/'\n",
    "\n",
    "local_dir = '/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 10000000\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "\n",
    "X = nd.random_normal(shape=(num_examples, num_inputs))\n",
    "noise = 0.01 * nd.random_normal(shape=(num_examples,))\n",
    "y = real_fn(X) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_upload(X,y,target_folder,i):\n",
    "    file_name = '{}'.format(i)\n",
    "    local_path = os.path.join(local_dir,file_name)\n",
    "    mx.nd.save(local_path,{'X':X, 'y':y})\n",
    "    print('Number of examples {}'.format(len(X)))\n",
    "    print('Created local file {}'.format(local_path))\n",
    "    upload_filename = '{}/{}/{}'.format(prefix,target_folder,file_name)\n",
    "    print('Uploading to {}'.format(upload_filename))\n",
    "    s3.upload_file(local_path, bucket_name, upload_filename)\n",
    "\n",
    "def split_and_upload(X,y,k,target_folder):\n",
    "    n = len(X)\n",
    "    assert (n//k)*k == n\n",
    "    idx = list(range(0, n+1, n//k))\n",
    "    X_shards = [X[idx[i]:idx[i+1]] for i in range(k)]\n",
    "    y_shards = [y[idx[i]:idx[i+1]] for i in range(k)]\n",
    "    \n",
    "    for Xi,yi,i in zip(X_shards,y_shards,range(k)):\n",
    "        save_and_upload(Xi,yi,target_folder,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000000\n",
      "1000000\n",
      "Number of examples 1800000\n",
      "Created local file /tmp/0\n",
      "Uploading to distributed_training_gluon_lab//train/0\n",
      "Number of examples 1800000\n",
      "Created local file /tmp/1\n",
      "Uploading to distributed_training_gluon_lab//train/1\n",
      "Number of examples 1800000\n",
      "Created local file /tmp/2\n",
      "Uploading to distributed_training_gluon_lab//train/2\n",
      "Number of examples 1800000\n",
      "Created local file /tmp/3\n",
      "Uploading to distributed_training_gluon_lab//train/3\n",
      "Number of examples 1800000\n",
      "Created local file /tmp/4\n",
      "Uploading to distributed_training_gluon_lab//train/4\n",
      "Number of examples 1000000\n",
      "Created local file /tmp/0\n",
      "Uploading to distributed_training_gluon_lab//test/0\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.9\n",
    "split_index = int(num_examples*train_frac)\n",
    "X_train = X[0:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_train = y[0:split_index]\n",
    "y_test = y[split_index:]\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "split_and_upload(X_train,y_train,5,'train')\n",
    "save_and_upload(X_test,y_test,'test',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_regression import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file path ./data/0\n",
      "Number of examples 100\n",
      "kvstore device\n",
      "Epoch 0, loss: 0.202694, time 0.007264 s\n",
      "Epoch 1, loss: 0.151727, time 0.005441 s\n",
      "Epoch 2, loss: 0.119578, time 0.005571 s\n",
      "Epoch 3, loss: 0.083341, time 0.004761 s\n",
      "Epoch 4, loss: 0.061990, time 0.004949 s\n",
      "Epoch 5, loss: 0.044581, time 0.004703 s\n",
      "Epoch 6, loss: 0.034423, time 0.004912 s\n",
      "Epoch 7, loss: 0.024573, time 0.004760 s\n",
      "Epoch 8, loss: 0.018596, time 0.004776 s\n",
      "Epoch 9, loss: 0.013838, time 0.004443 s\n",
      "Training complete in 0.053098 seconds\n"
     ]
    }
   ],
   "source": [
    "channel_input_dirs = {'train':'./data'}\n",
    "hyperparameters = {'batch_size':64, 'epochs':10, 'learning_rate':0.1}\n",
    "train(hyperparameters=hyperparameters,channel_input_dirs=channel_input_dirs,num_gpus=1,hosts=['alg-1'],\n",
    "      current_host='alg-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_estimator_10 = MXNet(entry_point='multiple_regression.py',\n",
    "    role=role,\n",
    "    train_instance_count=5, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters={'batch_size':512, 'epochs':10, 'learning_rate':0.0000001})\n",
    "\n",
    "mnist_estimator_1 = MXNet(entry_point='multiple_regression.py',\n",
    "    role=role,\n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters={'batch_size':512, 'epochs':10, 'learning_rate':0.0000001})\n",
    "\n",
    "train_data_location = 's3://{}/{}train'.format(bucket_name,prefix)\n",
    "test_data_location = 's3://{}/{}test'.format(bucket_name,prefix)\n",
    "\n",
    "mnist_estimator_1.fit({'train': train_data_location, 'test': test_data_location},wait=False)\n",
    "mnist_estimator_10.fit({'train': train_data_location, 'test': test_data_location},wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
