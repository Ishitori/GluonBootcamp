{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "import numpy as np\n",
    "from mxnet import gluon, autograd\n",
    "import time\n",
    "from mxnet import nd\n",
    "from gluonnlp.data import batchify as bf\n",
    "import mxnet as mx\n",
    "from gluonnlp.data.utils import slice_sequence, concat_sequence, _slice_pad_length\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from mxnet.gluon.data import SimpleDataset\n",
    "from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url\n",
    "from gluonnlp.data.registry import register\n",
    "''' \n",
    "When passing keyword arguments to `register`, they are checked to be valid keyword arguments for the registered \n",
    "Dataset class constructor and are saved in the registry.'''\n",
    "from gluonnlp.data.utils import _get_home_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/data/registry.py:91: UserWarning: \u001b[91mNew dataset __main__.TimeMachineDataLoader registered with name timemachinedataloader isoverriding existing dataset __main__.TimeMachineDataLoader\u001b[0m\n",
      "  return register_(class_)\n"
     ]
    }
   ],
   "source": [
    "@register(segment=['train', 'test'])\n",
    "class TimeMachineDataLoader(SimpleDataset):\n",
    "    def __init__(self, segment='train', \n",
    "                 root=os.path.join(_get_home_dir(), 'data', 'word_generator')):\n",
    "        self._data_file = {'train': ('train.txt', '5a84368fee37b6e38dc3c8c4e0365d32'),\n",
    "                          'test': ('test.txt', '48f4d966d69005504c13815cc2a777d0')}\n",
    "        root = os.path.expanduser(root)\n",
    "        if not os.path.isdir(root):\n",
    "            os.makedirs(root)\n",
    "        self._root = root\n",
    "        self._segment = segment\n",
    "        self._get_data()\n",
    "        self._file_path = self._get_file_path()\n",
    "        \n",
    "        super(TimeMachineDataLoader, self).__init__(self._read_data())\n",
    "        \n",
    "    @property\n",
    "    def file_path(self):\n",
    "        return self._file_path\n",
    "    \n",
    "    def _get_data(self):\n",
    "        data_file_name, data_hash = self._data_file[self._segment]\n",
    "        root = self._root\n",
    "        path = os.path.join(root, data_file_name)\n",
    "        if not os.path.exists(path) or not check_sha1(path, data_hash):\n",
    "            download('http://archive.org/stream/thetimemachine00035gut/35.txt', path=root)\n",
    "        with open(os.path.join(root, '35.txt')) as f:\n",
    "            raw_data = f.read()\n",
    "        raw_data = raw_data[44332: -24182]\n",
    "        raw_data_val = raw_data[-len(raw_data)//3:]\n",
    "        raw_data = raw_data[:2*len(raw_data)//3]\n",
    "        with open(os.path.join(root, 'train.txt'), 'w+') as output_file:\n",
    "            output_file.write(raw_data)\n",
    "        \n",
    "\n",
    "        with open(os.path.join(root, 'test.txt'), 'w+') as output_file:\n",
    "            output_file.write(raw_data_val)\n",
    "                \n",
    "\n",
    "    def _read_data(self):\n",
    "        with open(os.path.join(self._root, self._segment+'.txt')) as f:\n",
    "            samples = list(f.read())\n",
    "        return samples\n",
    "    \n",
    "    def _get_file_path(self):\n",
    "        data_file_name, data_hash = self._data_file[self._segment]\n",
    "        root = self._root\n",
    "        path = os.path.join(root, data_file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError\n",
    "        return path\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelDataSet(SimpleDataset):\n",
    "    def __init__(self, dataset, tokenizer=nlp.data.SpacyTokenizer('en')):\n",
    "        self._tokenizer = tokenizer\n",
    "        #self._dataset = self._tokenizer(dataset[:])\n",
    "        self._dataset = dataset\n",
    "    \n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self._dataset\n",
    "    \n",
    "    def batchify(self, vocab, batch_size):\n",
    "        data = self._dataset[:]\n",
    "        sample_len = len(data) // batch_size\n",
    "        return mx.nd.array(vocab[data[:sample_len * batch_size]]).reshape((batch_size, -1)).T\n",
    "    \n",
    "    def bptt_batchify(self, bptt, vocab, batch_size):\n",
    "        data = self.batchify(vocab, batch_size)\n",
    "        batches = slice_sequence(data, bptt+1, overlap=1)\n",
    "        return SimpleDataset(batches).transform(lambda x: (x[:min(len(x)-1, bptt), :], x[1:, :]))\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.idx_to_token)\n",
    "num_embd = 256\n",
    "num_hidden = 512\n",
    "num_layers = 3\n",
    "opt = 'sgd'\n",
    "lr = .001\n",
    "momentum = .9\n",
    "wd = 0\n",
    "num_gpus = min(16, mx.context.num_gpus())\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "batch_size = 64\n",
    "grad_clip = 0.25\n",
    "log_interval = 200\n",
    "model_name=\"CharLSTM\"\n",
    "dataset_name=\"TimeMachine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: (\n",
      "[[ 11.   5.   2. ...,   1.   1.   2.]\n",
      " [ 17.  19.  22. ...,  25.  11.  11.]\n",
      " [ 42.   1.  28. ...,   1.   2.  22.]\n",
      " ..., \n",
      " [ 23.   4.   5. ...,   1.  10.   8.]\n",
      " [  1.  12.  12. ...,  24.   2.   1.]\n",
      " [ 49.   6.  20. ...,   2.   2.  26.]]\n",
      "<NDArray 129x64 @cpu(0)>, \n",
      "[[ 17.  19.  22. ...,  25.  11.  11.]\n",
      " [ 42.   1.  28. ...,   1.   2.  22.]\n",
      " [  7.   4.   1. ...,  15.   2.   1.]\n",
      " ..., \n",
      " [  1.  12.  12. ...,  24.   2.   1.]\n",
      " [ 49.   6.  20. ...,   2.   2.  26.]\n",
      " [  6.   5.   1. ...,  19.   8.   2.]]\n",
      "<NDArray 129x64 @cpu(0)>)\n",
      "\n",
      "testdata: (\n",
      "[[ 13.   5.   9. ...,   5.   1.   2.]\n",
      " [ 20.   3.   2. ...,   1.  11.   1.]\n",
      " [  1.   1.   1. ...,  19.   7.   4.]\n",
      " ..., \n",
      " [ 12.  14.   2. ...,  12.   5.   3.]\n",
      " [  4.  19.  17. ...,   7.  11.   3.]\n",
      " [  3.   9.  16. ...,   5.   1.   2.]]\n",
      "<NDArray 129x64 @cpu(0)>, \n",
      "[[ 20.   3.   2. ...,   1.  11.   1.]\n",
      " [  1.   1.   1. ...,  19.   7.   4.]\n",
      " [ 13.   7.   7. ...,  12.   8.   5.]\n",
      " ..., \n",
      " [  4.  19.  17. ...,   7.  11.   3.]\n",
      " [  3.   9.  16. ...,   5.   1.   2.]\n",
      " [  7.   7.   4. ...,  19.   3.  10.]]\n",
      "<NDArray 129x64 @cpu(0)>)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = [TimeMachineDataLoader(segment=segment, root='../data/text_generator')\n",
    "                               for segment in ['train', 'test']]\n",
    "train_data = CharLevelDataSet(train_dataset)\n",
    "test_data = CharLevelDataSet(test_dataset)\n",
    "\n",
    "vocab = nlp.vocab.Vocab(nlp.data.Counter(train_dataset[:] + test_dataset[:]), \n",
    "                        padding_token=None, \n",
    "                        eos_token=None, \n",
    "                        bos_token=None)\n",
    "train_data, test_data = [x.bptt_batchify(bptt=129, vocab=vocab, batch_size=batch_size)\n",
    "                        for x in [train_data, test_data]]\n",
    "print(\"traindata: {}\\n\\ntestdata: {}\".format(train_data[0], test_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(gluon.Block):\n",
    "    def __init__(self, vocab_size, num_embd, num_hidden, num_layers, dropout=.5, **kwargs):\n",
    "        super(LSTMModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = gluon.nn.Dropout(dropout)\n",
    "            self.encoder = gluon.nn.Embedding(vocab_size, num_embd, weight_initializer=mx.init.Uniform(.1))\n",
    "            self.lstm = gluon.rnn.LSTM(hidden_size=num_hidden, \n",
    "                                       num_layers=num_layers, \n",
    "                                       dropout=dropout, \n",
    "                                       input_size = num_embd)\n",
    "            self.decoder = gluon.nn.Dense(units=vocab_size, in_units=num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        #print(\"EMB_SHAPE: {}\".format(emb.shape))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        #print(\"OUTPUT_SHAPE_IN_MODEL: {}\".format(output.shape))\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.lstm.begin_state(*args, **kwargs)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstmmodel8_ (\n",
       "  Parameter lstmmodel8_embedding0_weight (shape=(68, 256), dtype=float32)\n",
       "  Parameter lstmmodel8_lstm0_l0_i2h_weight (shape=(2048, 256), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l0_h2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l0_i2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l0_h2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l1_i2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l1_h2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l1_i2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l1_h2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l2_i2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l2_h2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l2_i2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_lstm0_l2_h2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel8_dense0_weight (shape=(68, 512), dtype=float32)\n",
       "  Parameter lstmmodel8_dense0_bias (shape=(68,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size=len(vocab.idx_to_token), num_embd=256, num_hidden=512, num_layers=3)\n",
    "model.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr, 'momentum': momentum, 'wd': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        #print(\"DATA_SHAPE: {}\".format(data.shape))\n",
    "        target = target.reshape((-1, )).as_in_context(ctx)\n",
    "        #print(\"TARGET_SHAPE: {}\".format(target.shape))\n",
    "        output, hidden = model(data, hidden)\n",
    "        #print(\"OUTPUT_SHAPE: {}\".format(output.shape))\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output,target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.21955379853613"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=model, data_source=test_data, batch_size=batch_size,ctx=ctx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr, context):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx) \n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, \n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, \n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output, y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / X.size\n",
    "                    Ls.append(batch_L / X.size)\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L), \n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('../model/{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) / \n",
    "                            (time.time() - start_train_time)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] throughput 842.32 samples/s\n",
      "[Epoch 0] time cost 1.23s, valid loss 3.07, valid ppl 21.58\n",
      "test loss 3.07, test ppl 21.58\n",
      "Total training throughput 616.87 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model=model, train_data=train_data, val_data=test_data, test_data=test_data, epochs=1, lr=lr, context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
